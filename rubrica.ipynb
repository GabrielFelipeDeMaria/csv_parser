{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lido: 1200_x_1010_1.csv com 1000000 linhas\n",
      "Lido: 1200_x_1010_2.csv com 479805 linhas\n",
      "Salvo: C:\\AFL_GABRIEL\\csv_parser\\DATA_PARQUET\\sis\\1200_x_1010_1.parquet\n",
      "Salvo: C:\\AFL_GABRIEL\\csv_parser\\DATA_PARQUET\\sis\\1200_x_1010_2.parquet\n",
      "Conversão concluída!\n"
     ]
    }
   ],
   "source": [
    "# CSV to PARQUET\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Diretórios\n",
    "caminho_raiz = 'C:\\\\AFL_GABRIEL\\\\csv_parser\\\\DATA\\\\sis\\\\1200 x 1010'\n",
    "saida_parquet = 'C:\\\\AFL_GABRIEL\\\\csv_parser\\\\DATA_PARQUET\\\\sis'\n",
    "\n",
    "# Criar o diretório de saída, se não existir\n",
    "os.makedirs(saida_parquet, exist_ok=True)\n",
    "\n",
    "# Dicionário para armazenar DataFrames por nome base\n",
    "dfs_paginas = {}\n",
    "\n",
    "# Caminhar por todas as subpastas e arquivos\n",
    "for root, _, files in os.walk(caminho_raiz):\n",
    "    for arquivo in files:\n",
    "        if arquivo.endswith('.csv'):\n",
    "            caminho_arquivo = os.path.join(root, arquivo)\n",
    "\n",
    "            try:\n",
    "                # Lê o CSV com separador pipe e tipos como string\n",
    "                df = pd.read_csv(caminho_arquivo, delimiter='|', dtype=str)\n",
    "            except Exception as e:\n",
    "                print(f'Erro ao ler {caminho_arquivo}: {e}')\n",
    "                continue\n",
    "\n",
    "            # Extrai o nome base (antes de _pagina ou da extensão)\n",
    "            nome_base = arquivo.split('_pagina')[0].replace('.csv', '')\n",
    "\n",
    "            if nome_base not in dfs_paginas:\n",
    "                dfs_paginas[nome_base] = []\n",
    "\n",
    "            if not df.empty:\n",
    "                dfs_paginas[nome_base].append(df)\n",
    "                print(f'Lido: {arquivo} com {df.shape[0]} linhas')\n",
    "\n",
    "# Concatenar e salvar\n",
    "for nome_base, lista_dfs in dfs_paginas.items():\n",
    "    if not lista_dfs:\n",
    "        print(f'{nome_base} - Nenhum dado encontrado')\n",
    "        continue\n",
    "\n",
    "    df_final = pd.concat(lista_dfs, ignore_index=True) if len(lista_dfs) > 1 else lista_dfs[0]\n",
    "    caminho_saida = os.path.join(saida_parquet, f'{nome_base}.parquet')\n",
    "\n",
    "    try:\n",
    "        df_final.to_parquet(caminho_saida, index=False)\n",
    "        print(f'Salvo: {caminho_saida}')\n",
    "    except Exception as e:\n",
    "        print(f'Erro ao salvar {nome_base}.parquet: {e}')\n",
    "\n",
    "print('Conversão concluída!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lido: 1200 x 1010.xlsx - Página: Parte 1 com 1000000 linhas\n",
      "Lido: 1200 x 1010.xlsx - Página: Parte 2 com 479805 linhas\n",
      "1200 x 1010.xlsx - Páginas unidas com 1479805 linhas\n",
      "Salvo: C:\\AFL_GABRIEL\\csv_parser\\DATA_PARQUET\\gabarito\\1200 x 1010.xlsx.parquet\n",
      "Conversão concluída!\n"
     ]
    }
   ],
   "source": [
    "# XLSX to PARQUET\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    " \n",
    "# Diretório onde os XLSX estão localizados\n",
    "caminho_xlsx = 'C:\\\\AFL_GABRIEL\\\\csv_parser\\\\DATA\\\\gabarito\\\\1200 x 1010'  \n",
    "caminho_parquet = 'C:\\\\AFL_GABRIEL\\\\csv_parser\\\\DATA_PARQUET\\\\gabarito'  \n",
    " \n",
    "# Criar o diretório de saída, se não existir\n",
    "if not os.path.exists(caminho_parquet):\n",
    "    os.makedirs(caminho_parquet)\n",
    " \n",
    "# Lista para armazenar DataFrames das páginas\n",
    "dfs_paginas = {}\n",
    " \n",
    "# Iterar pelos arquivos XLSX no diretório\n",
    "for arquivo in os.listdir(caminho_xlsx):\n",
    "    if arquivo.endswith('.xlsx'):\n",
    "        # Caminho completo do arquivo XLSX\n",
    "        caminho_xlsx = os.path.join(caminho_xlsx, arquivo)\n",
    "       \n",
    "        try:\n",
    "            # Lê todas as planilhas do arquivo XLSX em um dicionário\n",
    "            sheets_dict = pd.read_excel(caminho_xlsx, sheet_name=None)\n",
    "        except Exception as e:\n",
    "            print(f'Erro ao ler {arquivo}: {e}')\n",
    "            continue\n",
    "       \n",
    "        # Extrai o nome base antes da parte de paginação, se houver\n",
    "        nome_base = arquivo.split('_pagina')[0]\n",
    "       \n",
    "        # Se ainda não tiver uma entrada para este nome_base, cria\n",
    "        if nome_base not in dfs_paginas:\n",
    "            dfs_paginas[nome_base] = []\n",
    "       \n",
    "        # Itera por todas as páginas do arquivo e adiciona os DataFrames\n",
    "        for nome_sheet, df in sheets_dict.items():\n",
    "            if not df.empty:\n",
    "                dfs_paginas[nome_base].append(df)\n",
    "                print(f'Lido: {arquivo} - Página: {nome_sheet} com {df.shape[0]} linhas')\n",
    " \n",
    "# Processar e unir os DataFrames por nome_base\n",
    "for nome_base, lista_dfs in dfs_paginas.items():\n",
    "    # Se houver mais de uma página, concatena os DataFrames\n",
    "    if len(lista_dfs) > 1:\n",
    "        df_final = pd.concat(lista_dfs, ignore_index=True)\n",
    "        print(f'{nome_base} - Páginas unidas com {df_final.shape[0]} linhas')\n",
    "    elif lista_dfs:\n",
    "        df_final = lista_dfs[0]\n",
    "    else:\n",
    "        print(f'{nome_base} - Nenhum dado encontrado')\n",
    "        continue\n",
    "   \n",
    "    # Caminho para salvar o arquivo Parquet\n",
    "    caminho_parquet = os.path.join(caminho_parquet, f'{nome_base}.parquet')\n",
    "   \n",
    "    # Tenta salvar em formato Parquet\n",
    "    try:\n",
    "        df_final.to_parquet(caminho_parquet, index=False)\n",
    "        print(f'Salvo: {caminho_parquet}')\n",
    "    except Exception as e:\n",
    "        print(f'Erro ao salvar {nome_base}.parquet: {e}')\n",
    " \n",
    "print('Conversão concluída!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lido: 1200_x_1010_1.parquet com 1000000 linhas\n",
      "Lido: 1200_x_1010_2.parquet com 479805 linhas\n",
      "Resumo com totais copiado para o clipboard com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Caminho onde estão os arquivos Parquet\n",
    "caminho_parquet = 'C:\\\\AFL_GABRIEL\\\\csv_parser\\\\DATA_PARQUET\\\\sis'\n",
    "\n",
    "# Lista para armazenar todos os DataFrames\n",
    "lista_df = []\n",
    "\n",
    "# Lê todos os arquivos .parquet da pasta\n",
    "for arquivo in os.listdir(caminho_parquet):\n",
    "    if arquivo.endswith('.parquet'):\n",
    "        caminho_arquivo = os.path.join(caminho_parquet, arquivo)\n",
    "        try:\n",
    "            df = pd.read_parquet(caminho_arquivo)\n",
    "            lista_df.append(df)\n",
    "            print(f'Lido: {arquivo} com {df.shape[0]} linhas')\n",
    "        except Exception as e:\n",
    "            print(f'Erro ao ler {arquivo}: {e}')\n",
    "\n",
    "# Junta todos os DataFrames em um só\n",
    "df_total = pd.concat(lista_df, ignore_index=True)\n",
    "\n",
    "# Agrupa por 'Código rubrica' para contar:\n",
    "# - total de ocorrências\n",
    "# - número de descrições distintas\n",
    "resumo = df_total.groupby('Código rubrica').agg(\n",
    "    Qtd_total=('Descrição da Rubrica', 'count'),\n",
    "    Qtd_descricoes_distintas=('Descrição da Rubrica', 'nunique')\n",
    ").reset_index()\n",
    "\n",
    "# Copia para o clipboard (Ctrl+V no Excel)\n",
    "resumo.to_clipboard(index=False, excel=True)\n",
    "print('Resumo com totais copiado para o clipboard com sucesso!')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
