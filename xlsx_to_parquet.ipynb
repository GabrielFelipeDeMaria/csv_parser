{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\AFL_GABRIEL\\csv_parser\\venv\\Lib\\site-packages\\openpyxl\\styles\\stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'ID'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\AFL_GABRIEL\\csv_parser\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'ID'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m nome_sheet, df \u001b[38;5;129;01min\u001b[39;00m sheets_dict.items():\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m df.empty:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m         df[\u001b[33m'\u001b[39m\u001b[33mano\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mID\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.astype(\u001b[38;5;28mstr\u001b[39m).str[\u001b[32m17\u001b[39m:\u001b[32m21\u001b[39m]\n\u001b[32m     40\u001b[39m         df = df[df[\u001b[33m'\u001b[39m\u001b[33mano\u001b[39m\u001b[33m'\u001b[39m] >= \u001b[33m'\u001b[39m\u001b[33m2018\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     42\u001b[39m         \u001b[38;5;66;03m# Verifica se ainda tem dados após o filtro\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\AFL_GABRIEL\\csv_parser\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\AFL_GABRIEL\\csv_parser\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'ID'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    " \n",
    "# Diretório onde os XLSX estão localizados\n",
    "caminho_xlsx = 'C:\\\\AFL_GABRIEL\\\\csv_parser\\\\NF-e\\\\gabarito'  \n",
    "caminho_parquet = 'C:\\\\AFL_GABRIEL\\\\csv_parser\\\\NF-e Parquet\\\\gabarito'  \n",
    " \n",
    "# Criar o diretório de saída, se não existir\n",
    "if not os.path.exists(caminho_parquet):\n",
    "    os.makedirs(caminho_parquet)\n",
    " \n",
    "# Lista para armazenar DataFrames das páginas\n",
    "dfs_paginas = {}\n",
    " \n",
    "# Iterar pelos arquivos XLSX no diretório\n",
    "for arquivo in os.listdir(caminho_xlsx):\n",
    "    if arquivo.endswith('.xlsx'):\n",
    "        # Caminho completo do arquivo XLSX\n",
    "        caminho_xlsx = os.path.join(caminho_xlsx, arquivo)\n",
    "       \n",
    "        try:\n",
    "            # Lê todas as planilhas do arquivo XLSX em um dicionário\n",
    "            sheets_dict = pd.read_excel(caminho_xlsx, sheet_name=None)\n",
    "        except Exception as e:\n",
    "            print(f'Erro ao ler {arquivo}: {e}')\n",
    "            continue\n",
    "       \n",
    "        # Extrai o nome base antes da parte de paginação, se houver\n",
    "        nome_base = arquivo.split('_pagina')[0]\n",
    "       \n",
    "        # Se ainda não tiver uma entrada para este nome_base, cria\n",
    "        if nome_base not in dfs_paginas:\n",
    "            dfs_paginas[nome_base] = []\n",
    "       \n",
    "        # Itera por todas as páginas do arquivo e adiciona os DataFrames\n",
    "        for nome_sheet, df in sheets_dict.items():\n",
    "            if not df.empty:\n",
    "                df['ano'] = df['ID'].astype(str).str[17:21]\n",
    "               \n",
    "                df = df[df['ano'] >= '2018']\n",
    "               \n",
    "                # Verifica se ainda tem dados após o filtro\n",
    "                if not df.empty:\n",
    "                    dfs_paginas[nome_base].append(df)\n",
    "                    print(f'Lido: {arquivo} - Página: {nome_sheet} com {df.shape[0]} linhas após filtro')\n",
    "                else:\n",
    "                    print(f'{arquivo} - Página: {nome_sheet} sem dados válidos após filtro de ano')\n",
    " \n",
    "# Processar e unir os DataFrames por nome_base\n",
    "for nome_base, lista_dfs in dfs_paginas.items():\n",
    "    # Se houver mais de uma página, concatena os DataFrames\n",
    "    if len(lista_dfs) > 1:\n",
    "        df_final = pd.concat(lista_dfs, ignore_index=True)\n",
    "        print(f'{nome_base} - Páginas unidas com {df_final.shape[0]} linhas')\n",
    "    elif lista_dfs:\n",
    "        df_final = lista_dfs[0]\n",
    "    else:\n",
    "        print(f'{nome_base} - Nenhum dado encontrado')\n",
    "        continue\n",
    "   \n",
    "    # Caminho para salvar o arquivo Parquet\n",
    "    caminho_parquet = os.path.join(caminho_parquet, f'{nome_base}.parquet')\n",
    "   \n",
    "    # Tenta salvar em formato Parquet\n",
    "    try:\n",
    "        df_final.to_parquet(caminho_parquet, index=False)\n",
    "        print(f'Salvo: {caminho_parquet}')\n",
    "    except Exception as e:\n",
    "        print(f'Erro ao salvar {nome_base}.parquet: {e}')\n",
    " \n",
    "print('Conversão concluída!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Resultado copiado para a área de transferência!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyperclip\n",
    "\n",
    "# Caminho do arquivo Parquet\n",
    "caminho_parquet = 'C:\\\\AFL_GABRIEL\\\\csv_parser\\\\DATA\\\\sis\\\\s1200'\n",
    "\n",
    "# Lista de palavras-chave para conversão e substituição\n",
    "palavras_chave = [\"Vlr\", \"Valor\", \"vlr\", \"valor\", \"Vr\", \"vr\", \n",
    "                  \"base\", \"Base\", \"Salário\", \"Salario\", \"salário\", \n",
    "                  \"salario\", \"Provento\", \"provento\", \"Desconto\", \"desconto\", \n",
    "                  \"diferenca\", \"Diferenca\", \"diferença\", \"Diferença\"]\n",
    "\n",
    "# Ler o arquivo Parquet\n",
    "df = pd.read_parquet(caminho_parquet)\n",
    "\n",
    "# Informações do DataFrame\n",
    "num_linhas, num_colunas = df.shape  # Obter número de linhas e colunas\n",
    "\n",
    "# Criar a string com os resultados\n",
    "resultado = f\"🔹 Total de Linhas: {num_linhas}\\n🔹 Total de Colunas: {num_colunas}\\n\\n📊 Detalhes das Colunas:\\n\"\n",
    "\n",
    "# Percorre todas as colunas e adiciona detalhes\n",
    "for coluna in df.columns:\n",
    "    linhas_preenchidas = df[coluna].count()  # Conta valores não nulos\n",
    "    \n",
    "    # Verifica se a coluna está na lista de palavras-chave\n",
    "    if any(palavra.lower() in coluna.lower() for palavra in palavras_chave):\n",
    "            # Converte para float, ignorando erros caso tenha valores inválidos\n",
    "            df[coluna] = (\n",
    "                df[coluna].astype(str)\n",
    "                .str.replace('.', '', regex=False)      # Remove milhar\n",
    "                .str.replace(',', '.', regex=False)     # Converte decimal\n",
    "                .str.extract(r'(\\d+\\.?\\d*)')[0]          # Extrai número\n",
    "                .astype(float)\n",
    "            )\n",
    "            # Soma\n",
    "            soma_valores = df[coluna].sum(skipna=True)\n",
    "\n",
    "            # Exibição em formato brasileiro\n",
    "            valor_formatado = f\"{soma_valores:,.2f}\".replace(\",\", \"X\").replace(\".\", \",\").replace(\"X\", \".\")\n",
    "\n",
    "            resultado += f\"\\n📌 Coluna: {coluna} (🔄 Convertida para Float e Substituição de '.' por ',')\\n\"\n",
    "            resultado += f\"   🔹 Linhas Preenchidas: {linhas_preenchidas}\\n\"\n",
    "            resultado += f\"   🔹 Total Somado: {soma_valores:,.2f}\\n\"\n",
    "    else:\n",
    "        resultado += f\"\\n📌 Coluna: {coluna}\\n\"\n",
    "        resultado += f\"   🔹 Linhas Preenchidas: {linhas_preenchidas}\\n\"\n",
    "\n",
    "# Copiar para a área de transferência\n",
    "pyperclip.copy(resultado)\n",
    "\n",
    "# Exibir os resultados\n",
    "print(\"\\n✅ Resultado copiado para a área de transferência!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
