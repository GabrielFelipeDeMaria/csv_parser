{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotADirectoryError",
     "evalue": "[WinError 267] O nome do diretório é inválido: 'C:\\\\AFL_GABRIEL\\\\csv_parser\\\\SPEDxEFD\\\\MA\\\\csv\\\\EFD - Contribuições - 037 - Entradas - Todos os Registros - Completo.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m lista_dfs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Iterar sobre os arquivos CSV\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arquivo \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcaminho_csv\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m arquivo\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     24\u001b[0m         caminho_arquivo \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(caminho_csv, arquivo)\n",
      "\u001b[1;31mNotADirectoryError\u001b[0m: [WinError 267] O nome do diretório é inválido: 'C:\\\\AFL_GABRIEL\\\\csv_parser\\\\SPEDxEFD\\\\MA\\\\csv\\\\EFD - Contribuições - 037 - Entradas - Todos os Registros - Completo.csv'"
     ]
    }
   ],
   "source": [
    "# CSV TO PARQUET SISTEMA\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Caminhos de entrada e saída\n",
    "caminho_csv = 'C:\\\\AFL_GABRIEL\\\\csv_parser\\\\SPEDxEFD\\\\MA\\\\csv\\\\EFD - Contribuições - 037 - Entradas - Todos os Registros - Completo.csv'\n",
    "caminho_parquet = 'C:\\\\AFL_GABRIEL\\\\csv_parser\\\\SPEDxEFD\\\\MA\\\\parquet'\n",
    "\n",
    "# Nome do arquivo final Parquet\n",
    "nome_arquivo_parquet = '037.parquet'\n",
    "caminho_arquivo_parquet = os.path.join(caminho_parquet, nome_arquivo_parquet)\n",
    "\n",
    "# Criar o diretório de saída, se necessário\n",
    "os.makedirs(caminho_parquet, exist_ok=True)\n",
    "\n",
    "# Lista para armazenar os DataFrames\n",
    "lista_dfs = []\n",
    "\n",
    "# Iterar sobre os arquivos CSV\n",
    "for arquivo in os.listdir(caminho_csv):\n",
    "    if arquivo.endswith('.csv'):\n",
    "        caminho_arquivo = os.path.join(caminho_csv, arquivo)\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(caminho_arquivo, delimiter=\"|\", dtype=str)\n",
    "            if not df.empty:\n",
    "                lista_dfs.append(df)\n",
    "                print(f'Lido: {arquivo} com {df.shape[0]} linhas')\n",
    "            else:\n",
    "                print(f'Ignorado (vazio): {arquivo}')\n",
    "        except Exception as e:\n",
    "            print(f'Erro ao ler {arquivo}: {e}')\n",
    "\n",
    "# Verifica se há dados para salvar\n",
    "if lista_dfs:\n",
    "    df_final = pd.concat(lista_dfs, ignore_index=True)\n",
    "    try:\n",
    "        df_final.to_parquet(caminho_arquivo_parquet, index=False)\n",
    "        print(f'Dados salvos em: {caminho_arquivo_parquet} ({df_final.shape[0]} linhas)')\n",
    "    except Exception as e:\n",
    "        print(f'Erro ao salvar o arquivo Parquet: {e}')\n",
    "else:\n",
    "    print('Nenhum dado válido encontrado. Nenhum arquivo Parquet foi criado.')\n",
    "\n",
    "print('Conversão concluída!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lido: EFD - Contribuições - 037 - Entradas - Todos os Registros - Completo.csv com 16193 linhas\n",
      "Dados salvos em: C:\\AFL_GABRIEL\\csv_parser\\SPEDxEFD\\MA\\parquet\\037.parquet (16193 linhas)\n",
      "Conversão concluída!\n"
     ]
    }
   ],
   "source": [
    "# CSV TO PARQUET MA\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Caminhos de entrada e saída\n",
    "caminho_csv = 'C:\\\\AFL_GABRIEL\\\\csv_parser\\\\SPEDxEFD\\\\MA\\\\csv\\\\CONSOLID_EFD_CONTRIBUICAO_ENTRADAS_TODOS_REGISTROS_COMPLETO'\n",
    "caminho_parquet = 'C:\\\\AFL_GABRIEL\\\\csv_parser\\\\SPEDxEFD\\\\MA\\\\parquet'\n",
    "\n",
    "# Nome do arquivo final Parquet\n",
    "nome_arquivo_parquet = '037.parquet'\n",
    "caminho_arquivo_parquet = os.path.join(caminho_parquet, nome_arquivo_parquet)\n",
    "\n",
    "# Criar o diretório de saída, se necessário\n",
    "os.makedirs(caminho_parquet, exist_ok=True)\n",
    "\n",
    "# Lista para armazenar os DataFrames\n",
    "lista_dfs = []\n",
    "\n",
    "# Iterar sobre os arquivos CSV\n",
    "for arquivo in os.listdir(caminho_csv):\n",
    "    if arquivo.endswith('.csv'):\n",
    "        caminho_arquivo = os.path.join(caminho_csv, arquivo)\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(caminho_arquivo, delimiter=\";\", dtype=str)\n",
    "            if not df.empty:\n",
    "                lista_dfs.append(df)\n",
    "                print(f'Lido: {arquivo} com {df.shape[0]} linhas')\n",
    "            else:\n",
    "                print(f'Ignorado (vazio): {arquivo}')\n",
    "        except Exception as e:\n",
    "            print(f'Erro ao ler {arquivo}: {e}')\n",
    "\n",
    "# Verifica se há dados para salvar\n",
    "if lista_dfs:\n",
    "    df_final = pd.concat(lista_dfs, ignore_index=True)\n",
    "    try:\n",
    "        df_final.to_parquet(caminho_arquivo_parquet, index=False)\n",
    "        print(f'Dados salvos em: {caminho_arquivo_parquet} ({df_final.shape[0]} linhas)')\n",
    "    except Exception as e:\n",
    "        print(f'Erro ao salvar o arquivo Parquet: {e}')\n",
    "else:\n",
    "    print('Nenhum dado válido encontrado. Nenhum arquivo Parquet foi criado.')\n",
    "\n",
    "print('Conversão concluída!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "Error creating dataset. Could not read schema from 'C:/AFL_GABRIEL/csv_parser/DATA/diretos/ECF_M030_M300_M305_M310_LUCRO_REAL_LCTOS_PARTE_A_B_DO_E_LALUR/ECF - M030, M300, M305, M310 - Lucro Real - Lançamentos Parte A e B do e-Lalur.xlsx'. Is this a 'parquet' file?: Could not open Parquet input source 'C:/AFL_GABRIEL/csv_parser/DATA/diretos/ECF_M030_M300_M305_M310_LUCRO_REAL_LCTOS_PARTE_A_B_DO_E_LALUR/ECF - M030, M300, M305, M310 - Lucro Real - Lançamentos Parte A e B do e-Lalur.xlsx': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Lê o arquivo Parquet\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcaminho_parquet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# FILTRO POR ANO\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# if \"Período\" in df.columns:\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m#     df[\"Ano\"] = (\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m \n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Informações iniciais\u001b[39;00m\n\u001b[0;32m     45\u001b[0m num_linhas, num_colunas \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32mc:\\Users\\GabrielFelipe\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\io\\parquet.py:509\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, **kwargs)\u001b[0m\n\u001b[0;32m    506\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    507\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[1;32m--> 509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\GabrielFelipe\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\io\\parquet.py:227\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[1;34m(self, path, columns, use_nullable_dtypes, dtype_backend, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m path_or_handle, handles, kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilesystem\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_path_or_handle(\n\u001b[0;32m    221\u001b[0m     path,\n\u001b[0;32m    222\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilesystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    223\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    224\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m )\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 227\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    230\u001b[0m     result \u001b[38;5;241m=\u001b[39m pa_table\u001b[38;5;241m.\u001b[39mto_pandas(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mto_pandas_kwargs)\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\GabrielFelipe\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pyarrow\\parquet\\core.py:1793\u001b[0m, in \u001b[0;36mread_table\u001b[1;34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001b[0m\n\u001b[0;32m   1787\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1788\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_legacy_dataset\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated as of pyarrow 15.0.0 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1789\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand will be removed in a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1790\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m   1792\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1793\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mParquetDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1794\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1795\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1798\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1799\u001b[0m \u001b[43m        \u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_dictionary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1800\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1801\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1802\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1803\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1804\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1805\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecryption_properties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecryption_properties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1806\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1807\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1808\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1809\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1810\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m   1811\u001b[0m     \u001b[38;5;66;03m# fall back on ParquetFile for simple cases when pyarrow.dataset\u001b[39;00m\n\u001b[0;32m   1812\u001b[0m     \u001b[38;5;66;03m# module is not available\u001b[39;00m\n\u001b[0;32m   1813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\GabrielFelipe\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pyarrow\\parquet\\core.py:1371\u001b[0m, in \u001b[0;36mParquetDataset.__init__\u001b[1;34m(self, path_or_paths, filesystem, schema, filters, read_dictionary, memory_map, buffer_size, partitioning, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, use_legacy_dataset)\u001b[0m\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m partitioning \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhive\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1368\u001b[0m     partitioning \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mHivePartitioning\u001b[38;5;241m.\u001b[39mdiscover(\n\u001b[0;32m   1369\u001b[0m         infer_dictionary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 1371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparquet_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\GabrielFelipe\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pyarrow\\dataset.py:794\u001b[0m, in \u001b[0;36mdataset\u001b[1;34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[0m\n\u001b[0;32m    783\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m    784\u001b[0m     schema\u001b[38;5;241m=\u001b[39mschema,\n\u001b[0;32m    785\u001b[0m     filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    790\u001b[0m     selector_ignore_prefixes\u001b[38;5;241m=\u001b[39mignore_prefixes\n\u001b[0;32m    791\u001b[0m )\n\u001b[0;32m    793\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_path_like(source):\n\u001b[1;32m--> 794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_filesystem_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_is_path_like(elem) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, FileInfo) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m source):\n",
      "File \u001b[1;32mc:\\Users\\GabrielFelipe\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pyarrow\\dataset.py:486\u001b[0m, in \u001b[0;36m_filesystem_dataset\u001b[1;34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[0m\n\u001b[0;32m    478\u001b[0m options \u001b[38;5;241m=\u001b[39m FileSystemFactoryOptions(\n\u001b[0;32m    479\u001b[0m     partitioning\u001b[38;5;241m=\u001b[39mpartitioning,\n\u001b[0;32m    480\u001b[0m     partition_base_dir\u001b[38;5;241m=\u001b[39mpartition_base_dir,\n\u001b[0;32m    481\u001b[0m     exclude_invalid_files\u001b[38;5;241m=\u001b[39mexclude_invalid_files,\n\u001b[0;32m    482\u001b[0m     selector_ignore_prefixes\u001b[38;5;241m=\u001b[39mselector_ignore_prefixes\n\u001b[0;32m    483\u001b[0m )\n\u001b[0;32m    484\u001b[0m factory \u001b[38;5;241m=\u001b[39m FileSystemDatasetFactory(fs, paths_or_selector, \u001b[38;5;28mformat\u001b[39m, options)\n\u001b[1;32m--> 486\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfactory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\GabrielFelipe\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pyarrow\\_dataset.pyx:3089\u001b[0m, in \u001b[0;36mpyarrow._dataset.DatasetFactory.finish\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\GabrielFelipe\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pyarrow\\error.pxi:155\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\GabrielFelipe\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pyarrow\\error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowInvalid\u001b[0m: Error creating dataset. Could not read schema from 'C:/AFL_GABRIEL/csv_parser/DATA/diretos/ECF_M030_M300_M305_M310_LUCRO_REAL_LCTOS_PARTE_A_B_DO_E_LALUR/ECF - M030, M300, M305, M310 - Lucro Real - Lançamentos Parte A e B do e-Lalur.xlsx'. Is this a 'parquet' file?: Could not open Parquet input source 'C:/AFL_GABRIEL/csv_parser/DATA/diretos/ECF_M030_M300_M305_M310_LUCRO_REAL_LCTOS_PARTE_A_B_DO_E_LALUR/ECF - M030, M300, M305, M310 - Lucro Real - Lançamentos Parte A e B do e-Lalur.xlsx': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyperclip\n",
    "import re\n",
    "\n",
    "# Caminho do arquivo Parquet\n",
    "caminho_parquet = 'C:\\\\AFL_GABRIEL\\\\csv_parser\\\\DATA\\\\diretos\\\\ECF_M030_M300_M305_M310_LUCRO_REAL_LCTOS_PARTE_A_B_DO_E_LALUR'\n",
    "\n",
    "# Lista de palavras-chave que indicam colunas de valor\n",
    "palavras_chave = [\n",
    "    \"Vlr\", \"Valor\", \"vlr\", \"valor\", \"Vr\", \"vr\", \n",
    "    \"base\", \"Base\", \"Salário\", \"Salario\", \"salário\", \n",
    "    \"salario\", \"Provento\", \"provento\", \"Desconto\", \"desconto\", \n",
    "    \"diferenca\", \"Diferenca\", \"diferença\", \"Diferença\"\n",
    "]\n",
    "\n",
    "# Função robusta para converter string em float\n",
    "def extrair_float(valor):\n",
    "    try:\n",
    "        if pd.isna(valor):\n",
    "            return 0.0\n",
    "        valor = str(valor).replace('.', '')  # Remove milhar\n",
    "        valor = str(valor).replace(',', '.')  # Converte decimal\n",
    "        if valor in ['', '-', '.', '-.']:\n",
    "            return 0.0\n",
    "        return float(valor)\n",
    "    except:\n",
    "        print(f\"[ERRO] Valor inválido: {valor}\")\n",
    "        return 0.0\n",
    "\n",
    "# Lê o arquivo Parquet\n",
    "df = pd.read_parquet(caminho_parquet)\n",
    "\n",
    "# FILTRO POR ANO\n",
    "# if \"Período\" in df.columns:\n",
    "#     df[\"Ano\"] = (\n",
    "#         df[\"Período\"]\n",
    "#         .astype(str)\n",
    "#         .str.extract(r'(\\d{4})')[0]\n",
    "#         .astype(float)\n",
    "#         .astype(\"Int64\")\n",
    "#     )\n",
    "#     df = df[~df[\"Ano\"].isin([2021])]\n",
    "\n",
    "# Informações iniciais\n",
    "num_linhas, num_colunas = df.shape\n",
    "resultado = f\"🔹 Total de Linhas: {num_linhas}\\n🔹 Total de Colunas: {num_colunas}\\n\\n📊 Detalhes das Colunas:\\n\"\n",
    "\n",
    "# Loop em todas as colunas\n",
    "for coluna in df.columns:\n",
    "    if coluna == \"Ano\":\n",
    "        continue  # pula a coluna auxiliar \"Ano\"\n",
    "    \n",
    "    linhas_preenchidas = df[coluna].count()\n",
    "    eh_coluna_valor = any(palavra.lower() in coluna.lower() for palavra in palavras_chave)\n",
    "\n",
    "    if eh_coluna_valor:\n",
    "        df[coluna + \"_convertido\"] = df[coluna]#.apply(extrair_float)\n",
    "        soma_valores = df[coluna + \"_convertido\"].sum()\n",
    "        valor_formatado = f\"{soma_valores:,.2f}\".replace(\",\", \"X\").replace(\".\", \",\").replace(\"X\", \".\")\n",
    "\n",
    "        resultado += f\"\\n📌 Coluna: {coluna} (Valor)\\n\"\n",
    "        resultado += f\"   🔹 Linhas Preenchidas: {linhas_preenchidas}\\n\"\n",
    "        resultado += f\"   🔹 Soma Total: R$ {valor_formatado}\\n\"\n",
    "    else:\n",
    "        resultado += f\"\\n📌 Coluna: {coluna}\\n\"\n",
    "        resultado += f\"   🔹 Linhas Preenchidas: {linhas_preenchidas}\\n\"\n",
    "\n",
    "# Copia o resultado para a área de transferência\n",
    "pyperclip.copy(resultado)\n",
    "\n",
    "# Exibe o resultado\n",
    "print(\"\\n✅ Resultado copiado para a área de transferência!\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'periodo_de_apuracao', 'numero_do_recibo', 'tipo_do_arquivo',\n",
       "       'numero_do_recibo_retificado', 'cpf_trabalhador',\n",
       "       'codigo_da_categoria_do_trabalhador',\n",
       "       'numero_de_inscricao_do_estabelecimento', 'codigo_lotacao',\n",
       "       'grau_de_exposicao_a_agentes_nocivos', 'codigo_rubrica',\n",
       "       'identificador_rubrica', 'valor_total'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('C:\\\\AFL_GABRIEL\\\\csv_parser\\\\DATA\\\\sis\\\\s1200')\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50174466.83"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['valor_total'].sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
