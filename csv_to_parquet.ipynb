{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lido: S-1200 - RemuneraÃ§Ã£o de Trabalhador vinculado ao Regime Geral de PrevidenciÃ¡rio Social_1.csv com 1000000 linhas\n",
      "Lido: S-1200 - RemuneraÃ§Ã£o de Trabalhador vinculado ao Regime Geral de PrevidenciÃ¡rio Social_2.csv com 479805 linhas\n",
      "Dados salvos em: C:\\AFL_GABRIEL\\csv_parser\\DATA_PARQUET\\sis\\1200\\1200.parquet (1479805 linhas)\n",
      "ConversÃ£o concluÃ­da!\n"
     ]
    }
   ],
   "source": [
    "# CSV TO PARQUET\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Caminhos de entrada e saÃ­da\n",
    "caminho_csv = 'C:\\\\AFL_GABRIEL\\\\csv_parser\\\\DATA\\\\sis\\\\1200'\n",
    "caminho_parquet = 'C:\\\\AFL_GABRIEL\\\\csv_parser\\\\DATA_PARQUET\\\\sis\\\\1200'\n",
    "\n",
    "# Nome do arquivo final Parquet\n",
    "nome_arquivo_parquet = '1200.parquet'\n",
    "caminho_arquivo_parquet = os.path.join(caminho_parquet, nome_arquivo_parquet)\n",
    "\n",
    "# Criar o diretÃ³rio de saÃ­da, se necessÃ¡rio\n",
    "os.makedirs(caminho_parquet, exist_ok=True)\n",
    "\n",
    "# Lista para armazenar os DataFrames\n",
    "lista_dfs = []\n",
    "\n",
    "# Iterar sobre os arquivos CSV\n",
    "for arquivo in os.listdir(caminho_csv):\n",
    "    if arquivo.endswith('.csv'):\n",
    "        caminho_arquivo = os.path.join(caminho_csv, arquivo)\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(caminho_arquivo, delimiter=\"|\", dtype=str)\n",
    "            if not df.empty:\n",
    "                lista_dfs.append(df)\n",
    "                print(f'Lido: {arquivo} com {df.shape[0]} linhas')\n",
    "            else:\n",
    "                print(f'Ignorado (vazio): {arquivo}')\n",
    "        except Exception as e:\n",
    "            print(f'Erro ao ler {arquivo}: {e}')\n",
    "\n",
    "# Verifica se hÃ¡ dados para salvar\n",
    "if lista_dfs:\n",
    "    df_final = pd.concat(lista_dfs, ignore_index=True)\n",
    "    try:\n",
    "        df_final.to_parquet(caminho_arquivo_parquet, index=False)\n",
    "        print(f'Dados salvos em: {caminho_arquivo_parquet} ({df_final.shape[0]} linhas)')\n",
    "    except Exception as e:\n",
    "        print(f'Erro ao salvar o arquivo Parquet: {e}')\n",
    "else:\n",
    "    print('Nenhum dado vÃ¡lido encontrado. Nenhum arquivo Parquet foi criado.')\n",
    "\n",
    "print('ConversÃ£o concluÃ­da!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XLSX TO PARQUET\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    " \n",
    "# DiretÃ³rio onde os XLSX estÃ£o localizados\n",
    "caminho_xlsx = 'C:\\\\AFL_GABRIEL\\\\csv_parser\\\\DATA\\\\gabarito\\\\1200x1010'  \n",
    "caminho_parquet = 'C:\\\\AFL_GABRIEL\\\\csv_parser\\\\DATA_PARQUET\\\\gabarito\\\\1200x1010'  \n",
    " \n",
    "# Criar o diretÃ³rio de saÃ­da, se nÃ£o existir\n",
    "if not os.path.exists(caminho_parquet):\n",
    "    os.makedirs(caminho_parquet)\n",
    " \n",
    "# Lista para armazenar DataFrames das pÃ¡ginas\n",
    "dfs_paginas = {}\n",
    " \n",
    "# Iterar pelos arquivos XLSX no diretÃ³rio\n",
    "for arquivo in os.listdir(caminho_xlsx):\n",
    "    if arquivo.endswith('.xlsx'):\n",
    "        # Caminho completo do arquivo XLSX\n",
    "        caminho_xlsx = os.path.join(caminho_xlsx, arquivo)\n",
    "       \n",
    "        try:\n",
    "            # LÃª todas as planilhas do arquivo XLSX em um dicionÃ¡rio\n",
    "            sheets_dict = pd.read_excel(caminho_xlsx, sheet_name=None)\n",
    "        except Exception as e:\n",
    "            print(f'Erro ao ler {arquivo}: {e}')\n",
    "            continue\n",
    "       \n",
    "        # Extrai o nome base antes da parte de paginaÃ§Ã£o, se houver\n",
    "        nome_base = arquivo.split('_pagina')[0]\n",
    "       \n",
    "        # Se ainda nÃ£o tiver uma entrada para este nome_base, cria\n",
    "        if nome_base not in dfs_paginas:\n",
    "            dfs_paginas[nome_base] = []\n",
    "       \n",
    "        # Itera por todas as pÃ¡ginas do arquivo e adiciona os DataFrames\n",
    "        for nome_sheet, df in sheets_dict.items():\n",
    "            if not df.empty:\n",
    "                df['ano'] = df['ID'].astype(str).str[17:21]\n",
    "               \n",
    "                df = df[df['ano'] >= '2018']\n",
    "               \n",
    "                # Verifica se ainda tem dados apÃ³s o filtro\n",
    "                if not df.empty:\n",
    "                    dfs_paginas[nome_base].append(df)\n",
    "                    print(f'Lido: {arquivo} - PÃ¡gina: {nome_sheet} com {df.shape[0]} linhas apÃ³s filtro')\n",
    "                else:\n",
    "                    print(f'{arquivo} - PÃ¡gina: {nome_sheet} sem dados vÃ¡lidos apÃ³s filtro de ano')\n",
    " \n",
    "# Processar e unir os DataFrames por nome_base\n",
    "for nome_base, lista_dfs in dfs_paginas.items():\n",
    "    # Se houver mais de uma pÃ¡gina, concatena os DataFrames\n",
    "    if len(lista_dfs) > 1:\n",
    "        df_final = pd.concat(lista_dfs, ignore_index=True)\n",
    "        print(f'{nome_base} - PÃ¡ginas unidas com {df_final.shape[0]} linhas')\n",
    "    elif lista_dfs:\n",
    "        df_final = lista_dfs[0]\n",
    "    else:\n",
    "        print(f'{nome_base} - Nenhum dado encontrado')\n",
    "        continue\n",
    "   \n",
    "    # Caminho para salvar o arquivo Parquet\n",
    "    caminho_parquet = os.path.join(caminho_parquet, f'{nome_base}.parquet')\n",
    "   \n",
    "    # Tenta salvar em formato Parquet\n",
    "    try:\n",
    "        df_final.to_parquet(caminho_parquet, index=False)\n",
    "        print(f'Salvo: {caminho_parquet}')\n",
    "    except Exception as e:\n",
    "        print(f'Erro ao salvar {nome_base}.parquet: {e}')\n",
    " \n",
    "print('ConversÃ£o concluÃ­da!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lido: 59eebdfcfcad4dd986d57223025f2185-0.parquet (1648 linhas)\n",
      "\n",
      "âœ… Resultado copiado para a Ã¡rea de transferÃªncia!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyperclip\n",
    "import os\n",
    "\n",
    "# Caminho da pasta com arquivos Parquet\n",
    "caminho_parquet = 'C:\\\\AFL_GABRIEL\\\\csv_parser\\\\DATA_PARQUET\\\\sis\\\\s2200'\n",
    "\n",
    "# Lista de palavras-chave para conversÃ£o e substituiÃ§Ã£o\n",
    "palavras_chave = [\"Vlr\", \"Valor\", \"vlr\", \"valor\"]  # Modifique conforme necessÃ¡rio\n",
    "\n",
    "# Lista para armazenar DataFrames\n",
    "lista_df = []\n",
    "\n",
    "# Percorrer a pasta e juntar todos os arquivos .parquet\n",
    "for arquivo in os.listdir(caminho_parquet):\n",
    "    if arquivo.endswith('.parquet'):\n",
    "        caminho_arquivo = os.path.join(caminho_parquet, arquivo)\n",
    "        try:\n",
    "            df_temp = pd.read_parquet(caminho_arquivo)\n",
    "            lista_df.append(df_temp)\n",
    "            print(f'Lido: {arquivo} ({df_temp.shape[0]} linhas)')\n",
    "        except Exception as e:\n",
    "            print(f'Erro ao ler {arquivo}: {e}')\n",
    "\n",
    "# Junta todos os DataFrames lidos\n",
    "df = pd.concat(lista_df, ignore_index=True)\n",
    "\n",
    "# InformaÃ§Ãµes do DataFrame\n",
    "num_linhas, num_colunas = df.shape  # Obter nÃºmero de linhas e colunas\n",
    "\n",
    "# Criar a string com os resultados\n",
    "resultado = f\"ðŸ”¹ Total de Linhas: {num_linhas}\\nðŸ”¹ Total de Colunas: {num_colunas}\\n\\nðŸ“Š Detalhes das Colunas:\\n\"\n",
    "\n",
    "# Percorre todas as colunas\n",
    "for coluna in df.columns:\n",
    "    linhas_preenchidas = df[coluna].count()  # Conta valores nÃ£o nulos\n",
    "    \n",
    "    # Verifica se a coluna estÃ¡ na lista de palavras-chave\n",
    "    if any(palavra.lower() in coluna.lower() for palavra in palavras_chave):\n",
    "            # Converte para float, ignorando erros caso tenha valores invÃ¡lidos\n",
    "            df[coluna] = (\n",
    "                df[coluna].astype(str)\n",
    "                .str.replace('.', '', regex=False)      # Remove milhar\n",
    "                .str.replace(',', '.', regex=False)     # Converte decimal\n",
    "                .str.extract(r'(\\d+\\.?\\d*)')[0]          # Extrai nÃºmero\n",
    "                .astype(float)\n",
    "            )\n",
    "            # Soma\n",
    "            soma_valores = df[coluna].sum(skipna=True)\n",
    "\n",
    "            # ExibiÃ§Ã£o em formato brasileiro\n",
    "            valor_formatado = f\"{soma_valores:,.2f}\".replace(\",\", \"X\").replace(\".\", \",\").replace(\"X\", \".\")\n",
    "\n",
    "            resultado += f\"\\nðŸ“Œ Coluna: {coluna} (ðŸ”„ Convertida para Float e SubstituiÃ§Ã£o de '.' por ',')\\n\"\n",
    "            resultado += f\"   ðŸ”¹ Linhas Preenchidas: {linhas_preenchidas}\\n\"\n",
    "            resultado += f\"   ðŸ”¹ Total Somado: {soma_valores:,.2f}\\n\"\n",
    "    else:\n",
    "        resultado += f\"\\nðŸ“Œ Coluna: {coluna}\\n\"\n",
    "        resultado += f\"   ðŸ”¹ Linhas Preenchidas: {linhas_preenchidas}\\n\"\n",
    "\n",
    "# Copiar para a Ã¡rea de transferÃªncia\n",
    "pyperclip.copy(resultado)\n",
    "\n",
    "# Exibir os resultados\n",
    "print(\"\\nâœ… Resultado copiado para a Ã¡rea de transferÃªncia!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
